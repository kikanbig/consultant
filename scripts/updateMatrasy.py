#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∞–ª–∏–∞—Å–æ–≤ –º–∞—Ç—Ä–∞—Å–æ–≤ —Å —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏
"""

import json
import re

# –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∑–∞–º–µ–Ω
PHONETIC_RULES = [
    ('–µ', '—ç'), ('—ç', '–µ'),  # –µ/—ç
    ('–∏', '—ã'), ('—ã', '–∏'),  # –∏/—ã
    ('–æ', '–∞'), ('–∞', '–æ'),  # –æ/–∞
    ('—ë', '–µ'), ('–µ', '—ë'),  # —ë/–µ
    ('–π', '–∏'), ('–∏', '–π'),  # –π/–∏
    ('–Ω–Ω', '–Ω'), ('–Ω', '–Ω–Ω'),  # –¥–≤–æ–π–Ω—ã–µ —Å–æ–≥–ª–∞—Å–Ω—ã–µ
    ('–ª–ª', '–ª'), ('–ª', '–ª–ª'),
    ('–º–º', '–º'), ('–º', '–º–º'),
    ('—Å—Å', '—Å'), ('—Å', '—Å—Å'),
    ('—Ç—Ç', '—Ç'), ('—Ç', '—Ç—Ç'),
]

# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
TRANSLIT_MAP = {
    # Lagoma –º–æ–¥–µ–ª–∏
    'alma': ['–∞–ª—å–º–∞', '–∞–ª–º–∞', '–∞—É–º–∞', '–∞–ª—å–º–æ', '–∞–ª–º–æ', '–æ–ª–º–∞', '–æ—É–º–∞'],
    'asker': ['–∞—Å–∫–µ—Ä', '–∞—Å–∫—ç—Ä', '–æ—Å–∫–µ—Ä', '—ç—Å–∫–µ—Ä', '—ç—Å–∫–∞—Ä–∞', '–∞—Å–∫–∞—Ä', '–æ—Å–∫–∞—Ä', '—ç—Å–∫–∞—Ä', '–∞—Å–∫–∏—Ä'],
    'glatta': ['–≥–ª–∞—Ç—Ç–∞', '–≥–ª–∞—Ç–∞', '–≥–ª–∞—Ç—Ç–æ', '–≥–ª–∞—Ç–æ', '–≥–ª–ª–∞—Ç–∞', '–≥–ª–æ—Ç–∞'],
    'ilta': ['–∏–ª—å—Ç–∞', '–∏–ª—Ç–∞', '–∏–ª—å–¥–∞', '–∏–ª–¥–∞', '—ã–ª—Ç–∞'],
    'lenvik': ['–ª–µ–Ω–≤–∏–∫', '–ª–µ–Ω–≤—ñ–∫', '–ª—ç–Ω–≤–∏–∫', '–ª–µ–Ω–≤–∏–≥', '–ª–∞–Ω–≤–∏–∫', '–ª–µ–Ω–≤—ã–∫', '–ª–µ–Ω–≤–∏—Ü'],
    'lund': ['–ª—É–Ω–¥', '–ª–∞–Ω–¥', '–ª—É–Ω—Ç', '–ª—É–Ω—Ç—Ç', '–ª—É–Ω–Ω–¥'],
    'narvik': ['–Ω–∞—Ä–≤–∏–∫', '–Ω–∞—Ä–≤—ñ–∫', '–Ω–æ—Ä–≤–∏–∫', '–Ω–∞—Ä–≤—ã–≥', '–Ω–æ—Ä–≤–∏–≥', '–Ω–∞—Ä–≤–∏–≥'],
    'ulvik': ['—É–ª—å–≤–∏–∫', '—É–ª–≤–∏–∫', '—É–ª—å–≤—ñ–∫', '—É–ª–≤—ñ–∫', '—É–ª—å–≤–∏–≥', '—É–ª–≤–∏–≥', '—É–ª—å–≤—ã–≥'],
    
    # Veluna –º–æ–¥–µ–ª–∏
    'laoma': ['–ª–∞–æ–º–∞', '–ª–∞–æ–º–æ', '–ª–æ–æ–º–∞', '–ª–∞–∞–º–æ'],
    'palato': ['–ø–∞–ª–∞—Ç–æ', '–ø–∞–ª–∞—Ç—Ç–æ', '–ø–∞–ª–∞—Ç–∞', '–ø–æ–ª–∞—Ç–æ', '–ø–∞–ª–æ—Ç—Ç–æ'],
    
    # –õ–∞—Ç–∏–Ω—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    'lanwick': ['–ª–µ–Ω–≤–∏–∫'],
    'lenvick': ['–ª–µ–Ω–≤–∏–∫'],
    'lanvik': ['–ª–µ–Ω–≤–∏–∫'],
    'lenwig': ['–ª–µ–Ω–≤–∏–∫'],
}

def generate_phonetic_variants(word):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—è —Å–ª–æ–≤–∞
    """
    variants = set([word])
    
    for old, new in PHONETIC_RULES:
        if old in word:
            variants.add(word.replace(old, new))
    
    return variants

def generate_model_aliases_enhanced(model_name, brand_name):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∞–ª–∏–∞—Å—ã –¥–ª—è –º–æ–¥–µ–ª–∏ —Å —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏
    """
    aliases = set()
    model_lower = model_name.lower().strip()
    brand_lower = brand_name.lower().strip()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
    aliases.add(model_lower)
    
    # –í–∞—Ä–∏–∞–Ω—Ç—ã –±—Ä–µ–Ω–¥–∞
    brand_variants = []
    if brand_lower == 'lagoma':
        brand_variants = ['–ª–∞–≥–æ–º–∞', '–ª–∞–≥—É–Ω–∞', '–ª–∞–≥–æ–Ω–∞', '–ª–æ–≥–æ–º–∞', '–ª–∞–≥–æ–º–æ']
    elif brand_lower == 'veluna':
        brand_variants = ['–≤–µ–ª—É–Ω–∞', '–≤–µ–ª—é–Ω–∞', '–∏–ª—É–Ω–∞', '–≤–∏–ª—É–Ω–∞', '–≤—ç–ª—É–Ω–∞']
    
    # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –º–æ–¥–µ–ª–∏
    for variant in generate_phonetic_variants(model_lower):
        aliases.add(variant)
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
    if model_lower in TRANSLIT_MAP:
        for translit in TRANSLIT_MAP[model_lower]:
            aliases.add(translit)
            # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
            for phonetic in generate_phonetic_variants(translit):
                aliases.add(phonetic)
    
    # –ò—â–µ–º –ª–∞—Ç–∏–Ω—Å–∫–∏–π –∫–ª—é—á –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞
    for lat_key, cyr_variants in TRANSLIT_MAP.items():
        if model_lower in cyr_variants:
            aliases.add(lat_key)
    
    # –ö–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å –±—Ä–µ–Ω–¥–æ–º
    for brand_var in brand_variants:
        aliases.add(f"{brand_var} {model_lower}")
        
        # –° —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è–º–∏
        if model_lower in TRANSLIT_MAP:
            for translit in TRANSLIT_MAP[model_lower]:
                aliases.add(f"{brand_var} {translit}")
    
    # –õ–∞—Ç–∏–Ω—Å–∫–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
    aliases.add(f"{brand_lower} {model_lower}")
    
    # –£–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–ª–∏–∞—Å—ã
    aliases = {a for a in aliases if len(a) >= 3}
    
    return sorted(list(aliases))

def update_matrasy_json(input_path, output_path):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç JSON —Ñ–∞–π–ª —Å –º–∞—Ç—Ä–∞—Å–∞–º–∏, –¥–æ–±–∞–≤–ª—è—è —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–∏–∞—Å—ã
    """
    print(f"üìñ –ß–∏—Ç–∞—é —Ñ–∞–π–ª: {input_path}")
    
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    updated_count = 0
    total_aliases_before = 0
    total_aliases_after = 0
    
    for matras in data['matrasy']:
        brand = matras['brand']
        model = matras['model']
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –∞–ª–∏–∞—Å—ã –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        old_aliases = matras.get('aliases', [])
        total_aliases_before += len(old_aliases)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∞–ª–∏–∞—Å—ã
        new_aliases = generate_model_aliases_enhanced(model, brand)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ —Å—Ç–∞—Ä—ã–º–∏ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —Ç–∞–º –µ—Å—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ)
        combined_aliases = list(set(old_aliases + new_aliases))
        combined_aliases.sort()
        
        matras['aliases'] = combined_aliases
        total_aliases_after += len(combined_aliases)
        updated_count += 1
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π JSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} –º–∞—Ç—Ä–∞—Å–æ–≤")
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–ª–∏–∞—Å–æ–≤:")
    print(f"   –ë—ã–ª–æ: {total_aliases_before}")
    print(f"   –°—Ç–∞–ª–æ: {total_aliases_after}")
    print(f"   –ü—Ä–∏—Ä–æ—Å—Ç: +{total_aliases_after - total_aliases_before} ({int((total_aliases_after / total_aliases_before - 1) * 100)}%)")
    
    # –ü—Ä–∏–º–µ—Ä—ã
    print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã (–ø–µ—Ä–≤—ã–µ 3):")
    for i, matras in enumerate(data['matrasy'][:3], 1):
        print(f"\n{i}. {matras['brand']} {matras['model']}")
        print(f"   –ê–ª–∏–∞—Å–æ–≤: {len(matras['aliases'])}")
        print(f"   –ü—Ä–∏–º–µ—Ä—ã: {', '.join(matras['aliases'][:10])}")
        if len(matras['aliases']) > 10:
            print(f"   ... –∏ –µ—â–µ {len(matras['aliases']) - 10} –∞–ª–∏–∞—Å–æ–≤")

if __name__ == '__main__':
    import os
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.dirname(script_dir)
    input_path = os.path.join(project_dir, 'src', 'data', 'matrasy.json')
    output_path = input_path  # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–æ—Ç –∂–µ —Ñ–∞–π–ª
    
    print("üöÄ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–ª–∏–∞—Å–æ–≤ –º–∞—Ç—Ä–∞—Å–æ–≤\n")
    update_matrasy_json(input_path, output_path)
    print("\n‚ú® –ì–æ—Ç–æ–≤–æ!")

